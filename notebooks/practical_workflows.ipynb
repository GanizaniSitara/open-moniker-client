{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Practical Workflows with Moniker\n\nFour business workflows using real data from the Moniker catalog:\n\n1. **Fixed Income Relative Value** -- yield curve construction, swap spreads, sovereign cross-market analysis\n2. **Credit Risk Dashboard** -- exposure aggregation, limit monitoring, breach detection\n3. **MBS Pool Analysis** -- agency comparison, prepayment trends, outlier detection\n4. **Discovery & Governance** -- catalog exploration, ownership audit, data quality gates"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T21:12:25.523691Z",
     "iopub.status.busy": "2026-02-11T21:12:25.523542Z",
     "iopub.status.idle": "2026-02-11T21:12:26.203921Z",
     "shell.execute_reply": "2026-02-11T21:12:26.202862Z"
    }
   },
   "outputs": [],
   "source": "import sys, os\nimport pandas as pd\n\n# moniker_client is pip-installed (pip install -e ~/open-moniker-client)\n# We only need path setup for the mock data adapters used in demo mode\nfor p in [\n    os.path.expanduser(\"~/open-moniker-svc/src\"),\n    os.path.expanduser(\"~/open-moniker-svc/external/moniker-data/src\"),\n]:\n    if p not in sys.path:\n        sys.path.insert(0, p)\n\nfrom moniker_client import (\n    Moniker, MonikerClient, ClientConfig,\n    CatalogReflector,\n)\n\nfrom moniker_data.adapters.oracle import MockOracleAdapter\nfrom moniker_data.adapters.snowflake import MockSnowflakeAdapter\nfrom moniker_data.adapters.rest import MockRestAdapter\nfrom moniker_data.adapters.excel import MockExcelAdapter\nfrom moniker_data.adapters.mssql import MockMssqlAdapter\n\nMockOracleAdapter()\nMockSnowflakeAdapter()\nMockRestAdapter()\nMockExcelAdapter()\nMockMssqlAdapter()\n\nclient = MonikerClient(config=ClientConfig(service_url=\"http://localhost:8050\"))\nreflector = CatalogReflector(client=client)\n\npd.options.display.float_format = \"{:,.2f}\".format\nprint(\"Client ready:\", client.config.service_url)"
  },
  {
   "cell_type": "markdown",
   "id": "2ktegr40yyo",
   "source": "---\n## Workflow 1: Fixed Income Relative Value\n\nCompare Treasury yields, sovereign spreads, and swap rates across the curve.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xbmavqy3mef",
   "source": "# Fetch fixed income data from Snowflake\nust_result = client.fetch(\"fixed_income.govies/treasury/ALL/ALL\", limit=10000)\nsov_result = client.fetch(\"fixed_income.govies/sovereign/ALL/ALL\", limit=10000)\nswap_result = client.fetch(\"rates.swap/USD/ALL\", limit=10000)\n\nust = pd.DataFrame(ust_result.data)\nsov = pd.DataFrame(sov_result.data)\nswaps = pd.DataFrame(swap_result.data)\n\nprint(f\"UST:       {ust.shape[0]} rows x {ust.shape[1]} cols\")\nprint(f\"Sovereign: {sov.shape[0]} rows x {sov.shape[1]} cols\")\nprint(f\"Swaps:     {swaps.shape[0]} rows x {swaps.shape[1]} cols\")\n\n# Filter to latest date\nust_latest = ust[ust[\"ASOF_DATE\"] == ust[\"ASOF_DATE\"].max()].copy()\nsov_latest = sov[sov[\"ASOF_DATE\"] == sov[\"ASOF_DATE\"].max()].copy()\nswap_latest = swaps[swaps[\"ASOF_DATE\"] == swaps[\"ASOF_DATE\"].max()].copy()\n\nprint(f\"\\nLatest dates: UST={ust['ASOF_DATE'].max()}, Sovereign={sov['ASOF_DATE'].max()}, Swaps={swaps['ASOF_DATE'].max()}\")\n\n# Build UST yield curve\ncurve = ust_latest[[\"TENOR\", \"YIELD\", \"DURATION\", \"CONVEXITY\"]].sort_values(\"TENOR\").reset_index(drop=True)\nprint(f\"\\nUST Yield Curve ({len(curve)} tenors)\")\nprint(curve.to_string(index=False))\n\n# Join swap rates and calculate swap spread\nswap_tenors = swap_latest[[\"TENOR\", \"PAR_RATE\"]].rename(columns={\"PAR_RATE\": \"SWAP_RATE\"})\ncombined = curve.merge(swap_tenors, on=\"TENOR\", how=\"left\")\ncombined[\"SWAP_SPREAD_BPS\"] = ((combined[\"SWAP_RATE\"] - combined[\"YIELD\"]) * 100).round(1)\n\nprint(f\"\\nCombined Curve with Swap Spreads\")\nprint(combined.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gbbqzrc5tvn",
   "source": "# --- Sovereign spread matrix: COUNTRY x TENOR showing SPREAD_VS_USD ---\nprint(\"Sovereign Spread vs UST (bps) by Country and Tenor\")\nspread_matrix = sov_latest.pivot_table(\n    values=\"SPREAD_VS_USD\", index=\"COUNTRY\", columns=\"TENOR\", aggfunc=\"mean\"\n)\nprint(spread_matrix.round(1).to_string())\n\n# --- Duration/Convexity ladder across UST curve ---\nprint(\"\\n\\nUST Duration & Convexity Ladder\")\nladder = ust_latest[[\"TENOR\", \"YIELD\", \"DURATION\", \"CONVEXITY\"]].sort_values(\"TENOR\").reset_index(drop=True)\nladder[\"DUR_x_CONV\"] = (ladder[\"DURATION\"] * ladder[\"CONVEXITY\"].abs()).round(3)\nprint(ladder.to_string(index=False))\n\n# --- 30-day yield change by tenor ---\nprint(\"\\n\\n30-Day Yield Change by Tenor (bps)\")\nfirst_date = ust[\"ASOF_DATE\"].min()\nlast_date = ust[\"ASOF_DATE\"].max()\n\nust_first = ust[ust[\"ASOF_DATE\"] == first_date][[\"TENOR\", \"YIELD\"]].rename(columns={\"YIELD\": \"YIELD_START\"})\nust_last = ust[ust[\"ASOF_DATE\"] == last_date][[\"TENOR\", \"YIELD\"]].rename(columns={\"YIELD\": \"YIELD_END\"})\n\nyield_chg = ust_first.merge(ust_last, on=\"TENOR\").sort_values(\"TENOR\").reset_index(drop=True)\nyield_chg[\"CHANGE_BPS\"] = ((yield_chg[\"YIELD_END\"] - yield_chg[\"YIELD_START\"]) * 100).round(1)\nprint(f\"Period: {first_date} to {last_date}\")\nprint(yield_chg.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n## Workflow 2: Credit Risk Dashboard\n\nAggregate counterparty exposures, join against approved limits, flag breaches."
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T21:12:26.205697Z",
     "iopub.status.busy": "2026-02-11T21:12:26.205456Z",
     "iopub.status.idle": "2026-02-11T21:12:26.352910Z",
     "shell.execute_reply": "2026-02-11T21:12:26.351973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exposures: 736 rows x 15 cols\n",
      "Limits:    32 rows x 7 cols\n",
      "\n",
      "Snapshot date: 2026-02-11 (32 rows)\n",
      "\n",
      "CP      Name                   Sector                     Notional            Limit   Util% Breach\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "CP001   Goldman Sachs Group    Financials            1,195,930,545      546,198,735  219.0% *** YES\n",
      "CP005   BHP Group Ltd          Mining                1,513,291,844      764,248,174  198.0% *** YES\n",
      "CP004   Shell plc              Energy                1,503,301,524      993,470,310  151.3% *** YES\n",
      "CP007   Samsung Electronics    Technology            1,086,110,703    1,293,877,805   83.9% \n",
      "CP003   Toyota Motor Corp      Automotive            1,146,351,628    1,676,219,440   68.4% \n",
      "CP002   Deutsche Bank AG       Financials            1,138,581,901    1,768,003,829   64.4% \n",
      "CP006   Petrobras SA           Energy                  960,453,110    1,559,681,511   61.6% \n",
      "CP008   Nestlé SA              Consumer Staples        724,408,633    1,616,479,245   44.8% \n",
      "\n",
      "Breaches: 3 of 8 counterparties\n"
     ]
    }
   ],
   "source": [
    "# Fetch credit data from MS-SQL\n",
    "exp_result = client.fetch(\"credit.exposures\", limit=10000)\n",
    "lim_result = client.fetch(\"credit.limits\", limit=10000)\n",
    "\n",
    "exp = pd.DataFrame(exp_result.data)\n",
    "lim = pd.DataFrame(lim_result.data)\n",
    "print(f\"Exposures: {len(exp)} rows x {len(exp.columns)} cols\")\n",
    "print(f\"Limits:    {len(lim)} rows x {len(lim.columns)} cols\")\n",
    "\n",
    "# Latest date snapshot\n",
    "latest_date = exp[\"ASOF_DATE\"].max()\n",
    "latest = exp[exp[\"ASOF_DATE\"] == latest_date].copy()\n",
    "print(f\"\\nSnapshot date: {latest_date} ({len(latest)} rows)\")\n",
    "\n",
    "# Aggregate by counterparty\n",
    "by_cp = (\n",
    "    latest\n",
    "    .groupby([\"COUNTERPARTY_ID\", \"COUNTERPARTY_NAME\", \"SECTOR\", \"RATING\"])\n",
    "    .agg(NOTIONAL=(\"NOTIONAL\", \"sum\"), CVA=(\"CVA\", \"sum\"), EXPECTED_LOSS=(\"EXPECTED_LOSS\", \"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Join with SingleName limits\n",
    "sn_limits = lim[lim[\"LIMIT_TYPE\"] == \"SingleName\"][[\"COUNTERPARTY_ID\", \"LIMIT_AMOUNT\"]].copy()\n",
    "dashboard = by_cp.merge(sn_limits, on=\"COUNTERPARTY_ID\", how=\"left\")\n",
    "\n",
    "# Utilization and breach detection\n",
    "dashboard[\"UTILIZATION_PCT\"] = (dashboard[\"NOTIONAL\"] / dashboard[\"LIMIT_AMOUNT\"] * 100).round(1)\n",
    "dashboard[\"BREACH\"] = dashboard[\"UTILIZATION_PCT\"] > 100\n",
    "dashboard = dashboard.sort_values(\"UTILIZATION_PCT\", ascending=False)\n",
    "\n",
    "print(f\"\\n{'CP':<7} {'Name':<22} {'Sector':<18} {'Notional':>16} {'Limit':>16} {'Util%':>7} {'Breach'}\")\n",
    "print(\"-\" * 108)\n",
    "for _, r in dashboard.iterrows():\n",
    "    flag = \"*** YES\" if r[\"BREACH\"] else \"\"\n",
    "    print(f\"{r['COUNTERPARTY_ID']:<7} {r['COUNTERPARTY_NAME']:<22} {r['SECTOR']:<18} \"\n",
    "          f\"{r['NOTIONAL']:>16,.0f} {r['LIMIT_AMOUNT']:>16,.0f} {r['UTILIZATION_PCT']:>6.1f}% {flag}\")\n",
    "\n",
    "breaches = dashboard[\"BREACH\"].sum()\n",
    "print(f\"\\nBreaches: {breaches} of {len(dashboard)} counterparties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T21:12:26.355310Z",
     "iopub.status.busy": "2026-02-11T21:12:26.354516Z",
     "iopub.status.idle": "2026-02-11T21:12:26.378441Z",
     "shell.execute_reply": "2026-02-11T21:12:26.377401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector Concentration\n",
      "Sector                       Notional  % Total\n",
      "----------------------------------------------\n",
      "Energy                  2,463,754,634    26.6%\n",
      "Financials              2,334,512,446    25.2%\n",
      "Mining                  1,513,291,844    16.3%\n",
      "Automotive              1,146,351,628    12.4%\n",
      "Technology              1,086,110,703    11.7%\n",
      "Consumer Staples          724,408,633     7.8%\n",
      "\n",
      "\n",
      "Rating x Exposure Type ($ millions)\n",
      "EXPOSURE_TYPE  Derivative  Guarantee   Loan  TradeFinance\n",
      "RATING                                                   \n",
      "A                  346.90     452.10 240.20        464.10\n",
      "A+                 782.40     898.20 588.00        391.00\n",
      "A-                 482.10     450.80  83.90        121.80\n",
      "AA                  19.20      47.40 302.00        355.70\n",
      "AA-                519.00     685.50 578.70        498.90\n",
      "BB-                424.20      90.90   6.50        438.80\n",
      "\n",
      "\n",
      "30-Day Exposure Summary by Counterparty\n",
      "                          min            max           mean            std  range_pct\n",
      "COUNTERPARTY_ID                                                                      \n",
      "CP001           16,110,789.00 499,498,569.00 267,743,251.00 142,185,626.00     180.00\n",
      "CP002            7,333,183.00 499,799,547.00 259,069,171.00 155,128,634.00     190.00\n",
      "CP003           24,576,039.00 487,426,259.00 284,323,790.00 131,207,141.00     163.00\n",
      "CP004            7,707,380.00 492,816,710.00 271,227,722.00 132,069,810.00     179.00\n",
      "CP005            6,224,177.00 498,904,623.00 240,936,770.00 140,566,544.00     204.00\n",
      "CP006            5,334,586.00 495,210,153.00 229,492,397.00 149,481,457.00     214.00\n",
      "CP007           11,477,624.00 491,809,395.00 237,799,869.00 134,262,556.00     202.00\n",
      "CP008           10,236,105.00 498,747,204.00 234,318,608.00 143,552,685.00     208.00\n"
     ]
    }
   ],
   "source": [
    "# --- Sector concentration ---\n",
    "sector = latest.groupby(\"SECTOR\")[\"NOTIONAL\"].sum().sort_values(ascending=False)\n",
    "sector_pct = (sector / sector.sum() * 100).round(1)\n",
    "\n",
    "print(\"Sector Concentration\")\n",
    "print(f\"{'Sector':<20} {'Notional':>16} {'% Total':>8}\")\n",
    "print(\"-\" * 46)\n",
    "for s, val in sector.items():\n",
    "    print(f\"{s:<20} {val:>16,.0f} {sector_pct[s]:>7.1f}%\")\n",
    "\n",
    "# --- Rating x ExposureType pivot (in millions) ---\n",
    "print(\"\\n\\nRating x Exposure Type ($ millions)\")\n",
    "pivot = (\n",
    "    latest\n",
    "    .pivot_table(values=\"NOTIONAL\", index=\"RATING\", columns=\"EXPOSURE_TYPE\",\n",
    "                 aggfunc=\"sum\", fill_value=0)\n",
    "    / 1_000_000\n",
    ")\n",
    "print(pivot.round(1).to_string())\n",
    "\n",
    "# --- 30-day exposure time series summary ---\n",
    "print(\"\\n\\n30-Day Exposure Summary by Counterparty\")\n",
    "ts = exp.groupby(\"COUNTERPARTY_ID\")[\"NOTIONAL\"].agg([\"min\", \"max\", \"mean\", \"std\"])\n",
    "ts[\"range_pct\"] = ((ts[\"max\"] - ts[\"min\"]) / ts[\"mean\"] * 100).round(1)\n",
    "print(ts.round(0).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "---\n## Workflow 3: MBS Pool Analysis\n\nAnalyze mortgage-backed securities pools across agencies and coupon types."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T21:12:26.381001Z",
     "iopub.status.busy": "2026-02-11T21:12:26.380825Z",
     "iopub.status.idle": "2026-02-11T21:12:26.419362Z",
     "shell.execute_reply": "2026-02-11T21:12:26.417889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBS pools: 270 rows x 21 cols\n",
      "Snapshot date: 2026-01-15 (45 rows)\n",
      "\n",
      "Agency Summary (as of 2026-01-15)\n",
      "        POOLS  TOTAL_BALANCE  AVG_CPR_1M  AVG_CPR_3M  AVG_OAS  AVG_DURATION  AVG_CONVEXITY\n",
      "AGENCY                                                                                    \n",
      "FHLMC       5     3346428191        9.55       10.65    56.23          5.48          -1.19\n",
      "FNMA        5     3735916105        9.18        9.78    49.69          5.41          -1.22\n",
      "GNMA        5     3484443637        9.29       10.07    53.28          5.45          -1.29\n",
      "\n",
      "\n",
      "Coupon Comparison (15Y vs 20Y vs 30Y)\n",
      "        POOLS  AVG_WAC  AVG_DURATION  AVG_OAS  AVG_CPR_1M  TOTAL_BALANCE\n",
      "COUPON                                                                  \n",
      "15Y        15     0.05          5.19    52.77        8.67     3872000872\n",
      "20Y        15     0.06          5.53    58.40       10.51     3395142312\n",
      "30Y        15     0.07          5.62    48.04        8.84     3299644749\n"
     ]
    }
   ],
   "source": [
    "# Fetch MBS pool data from Excel source\n",
    "mbs_result = client.fetch(\"reports/regulatory/2026Q1/summary\", limit=10000)\n",
    "mbs = pd.DataFrame(mbs_result.data)\n",
    "print(f\"MBS pools: {len(mbs)} rows x {len(mbs.columns)} cols\")\n",
    "\n",
    "# Latest snapshot\n",
    "latest_mbs_date = mbs[\"ASOF_DATE\"].max()\n",
    "snap = mbs[mbs[\"ASOF_DATE\"] == latest_mbs_date].copy()\n",
    "print(f\"Snapshot date: {latest_mbs_date} ({len(snap)} rows)\")\n",
    "\n",
    "# Agency-level aggregates\n",
    "agency_agg = snap.groupby(\"AGENCY\").agg(\n",
    "    POOLS=(\"POOL_ID\", \"nunique\"),\n",
    "    TOTAL_BALANCE=(\"CURRENT_BALANCE\", \"sum\"),\n",
    "    AVG_CPR_1M=(\"CPR_1M\", \"mean\"),\n",
    "    AVG_CPR_3M=(\"CPR_3M\", \"mean\"),\n",
    "    AVG_OAS=(\"OAS\", \"mean\"),\n",
    "    AVG_DURATION=(\"DURATION\", \"mean\"),\n",
    "    AVG_CONVEXITY=(\"CONVEXITY\", \"mean\"),\n",
    ")\n",
    "print(f\"\\nAgency Summary (as of {latest_mbs_date})\")\n",
    "print(agency_agg.round(2).to_string())\n",
    "\n",
    "# Coupon comparison: 15Y vs 20Y vs 30Y\n",
    "coupon_agg = snap.groupby(\"COUPON\").agg(\n",
    "    POOLS=(\"POOL_ID\", \"nunique\"),\n",
    "    AVG_WAC=(\"WAC\", \"mean\"),\n",
    "    AVG_DURATION=(\"DURATION\", \"mean\"),\n",
    "    AVG_OAS=(\"OAS\", \"mean\"),\n",
    "    AVG_CPR_1M=(\"CPR_1M\", \"mean\"),\n",
    "    TOTAL_BALANCE=(\"CURRENT_BALANCE\", \"sum\"),\n",
    ")\n",
    "print(\"\\n\\nCoupon Comparison (15Y vs 20Y vs 30Y)\")\n",
    "print(coupon_agg.round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T21:12:26.421101Z",
     "iopub.status.busy": "2026-02-11T21:12:26.420855Z",
     "iopub.status.idle": "2026-02-11T21:12:26.434999Z",
     "shell.execute_reply": "2026-02-11T21:12:26.434130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepayment Speed Trends (CPR_1M) by Agency\n",
      "AGENCY      FHLMC  FNMA  GNMA\n",
      "ASOF_DATE                    \n",
      "2025-08-18   9.76  8.54 10.10\n",
      "2025-09-17  10.03  8.69  9.39\n",
      "2025-10-17  10.27  8.87  8.86\n",
      "2025-11-16   9.93  9.13  9.11\n",
      "2025-12-16   9.74  8.39  9.26\n",
      "2026-01-15   9.55  9.18  9.29\n",
      "\n",
      "\n",
      "Balance Runoff (Original vs Current)\n",
      "          ORIGINAL     CURRENT  RUNOFF_PCT\n",
      "AGENCY                                    \n",
      "FHLMC   4956631613  3346428191       32.50\n",
      "FNMA    4875716500  3735916105       23.40\n",
      "GNMA    3903202644  3484443637       10.70\n",
      "\n",
      "\n",
      "Pools with Outlier CPR (>2 std dev from mean)\n",
      "Mean CPR_1M: 9.3,  Std: 2.6,  Thresholds: [4.1, 14.6]\n",
      "No outlier pools found.\n"
     ]
    }
   ],
   "source": [
    "# --- Prepayment speed trends over 6-month history by agency ---\n",
    "print(\"Prepayment Speed Trends (CPR_1M) by Agency\")\n",
    "trend = (\n",
    "    mbs.groupby([\"ASOF_DATE\", \"AGENCY\"])[\"CPR_1M\"]\n",
    "    .mean()\n",
    "    .unstack(\"AGENCY\")\n",
    "    .sort_index()\n",
    ")\n",
    "print(trend.round(2).to_string())\n",
    "\n",
    "# --- Balance runoff (ORIGINAL vs CURRENT) ---\n",
    "print(\"\\n\\nBalance Runoff (Original vs Current)\")\n",
    "runoff = snap.groupby(\"AGENCY\").agg(\n",
    "    ORIGINAL=(\"ORIGINAL_BALANCE\", \"sum\"),\n",
    "    CURRENT=(\"CURRENT_BALANCE\", \"sum\"),\n",
    ")\n",
    "runoff[\"RUNOFF_PCT\"] = ((1 - runoff[\"CURRENT\"] / runoff[\"ORIGINAL\"]) * 100).round(1)\n",
    "print(runoff.to_string())\n",
    "\n",
    "# --- Pools with outlier CPR (> 2 std dev from mean) ---\n",
    "print(\"\\n\\nPools with Outlier CPR (>2 std dev from mean)\")\n",
    "mean_cpr = snap[\"CPR_1M\"].mean()\n",
    "std_cpr = snap[\"CPR_1M\"].std()\n",
    "lo, hi = mean_cpr - 2 * std_cpr, mean_cpr + 2 * std_cpr\n",
    "\n",
    "outliers = snap[(snap[\"CPR_1M\"] > hi) | (snap[\"CPR_1M\"] < lo)].copy()\n",
    "outliers = outliers[[\"POOL_ID\", \"AGENCY\", \"COUPON\", \"CPR_1M\", \"CURRENT_BALANCE\"]]\n",
    "outliers = outliers.sort_values(\"CPR_1M\", ascending=False)\n",
    "\n",
    "print(f\"Mean CPR_1M: {mean_cpr:.1f},  Std: {std_cpr:.1f},  Thresholds: [{lo:.1f}, {hi:.1f}]\")\n",
    "if len(outliers) > 0:\n",
    "    print(outliers.to_string(index=False))\n",
    "else:\n",
    "    print(\"No outlier pools found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "---\n## Workflow 4: Discovery & Governance\n\nExplore the catalog, inspect before fetching, audit ownership and data quality."
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T21:12:26.436762Z",
     "iopub.status.busy": "2026-02-11T21:12:26.436599Z",
     "iopub.status.idle": "2026-02-11T21:12:26.471510Z",
     "shell.execute_reply": "2026-02-11T21:12:26.468892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog: 27 monikers across 7 source types\n",
      "Sources: {'snowflake': 3, 'rest': 4, 'oracle': 1, 'static': 1, 'opensearch': 1, 'excel': 1, 'mssql': 2}\n",
      "\n",
      "--- Search: 'credit' ---\n",
      "  credit                         source=?        tags=[]\n",
      "  credit.exposures               source=?        tags=['counterparty', 'timeseries', 'credit', 'mssql', 'risk']\n",
      "  credit.limits                  source=?        tags=['governance', 'limits', 'credit', 'mssql', 'risk']\n",
      "\n",
      "--- Schema: credit.exposures ---\n",
      "Granularity: daily per-counterparty per-exposure-type\n",
      "Primary key: ['ASOF_DATE', 'COUNTERPARTY_ID', 'EXPOSURE_TYPE']\n",
      "Columns (15):\n",
      "  ASOF_DATE              date       Business date of the exposure snapshot\n",
      "  COUNTERPARTY_ID        string     Unique counterparty identifier\n",
      "  COUNTERPARTY_NAME      string     Legal entity name\n",
      "  SECTOR                 string     Industry sector classification\n",
      "  RATING                 string     Credit rating (S&P scale)\n",
      "  COUNTRY                string     Country of incorporation (ISO 3166-1 alpha-2)\n",
      "  EXPOSURE_TYPE          string     Type of credit exposure\n",
      "  NOTIONAL               float      Notional amount of the exposure\n",
      "  MARK_TO_MARKET         float      Current mark-to-market value\n",
      "  PFE                    float      Potential Future Exposure at 97.5% confidence\n",
      "  CVA                    float      Credit Valuation Adjustment\n",
      "  LGD                    float      Loss Given Default (0-1)\n",
      "  PD                     float      Probability of Default (annualized)\n",
      "  EXPECTED_LOSS          float      Expected loss = Notional x LGD x PD\n",
      "  CURRENCY               string     Exposure currency (ISO 4217)\n",
      "\n",
      "Pattern: Discover -> Inspect -> Decide -> Fetch\n"
     ]
    }
   ],
   "source": [
    "# Step 1: What's in the catalog?\n",
    "stats = reflector.stats()\n",
    "print(f\"Catalog: {stats.total_monikers} monikers across {len(stats.by_source_type)} source types\")\n",
    "print(f\"Sources: {dict(stats.by_source_type)}\")\n",
    "\n",
    "# Step 2: Search for relevant data\n",
    "print(\"\\n--- Search: 'credit' ---\")\n",
    "results = reflector.search(\"credit\")\n",
    "for r in results.results:\n",
    "    print(f\"  {r['path']:<30} source={r.get('source_type', '?'):<8} tags={r.get('tags', [])}\")\n",
    "\n",
    "# Step 3: Inspect schema before fetching\n",
    "print(\"\\n--- Schema: credit.exposures ---\")\n",
    "schema = reflector.schema(\"credit.exposures\")\n",
    "print(f\"Granularity: {schema.granularity}\")\n",
    "print(f\"Primary key: {schema.primary_key}\")\n",
    "print(f\"Columns ({len(schema.columns)}):\")\n",
    "for col in schema.columns:\n",
    "    print(f\"  {col['name']:<22} {col.get('type',''):<10} {col.get('description', '')}\")\n",
    "\n",
    "print(\"\\nPattern: Discover -> Inspect -> Decide -> Fetch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T21:12:26.474192Z",
     "iopub.status.busy": "2026-02-11T21:12:26.473817Z",
     "iopub.status.idle": "2026-02-11T21:12:26.556329Z",
     "shell.execute_reply": "2026-02-11T21:12:26.555739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Domain Governance: credit ===\n",
      "  Owner:   credit-risk-governance@firm.com\n",
      "  Support: #credit-risk-data\n",
      "\n",
      "=== Lineage: credit.exposures ===\n",
      "  Owner:     credit-risk-governance@firm.com (from credit)\n",
      "  Hierarchy:  -> credit.exposures\n",
      "\n",
      "=== Data Quality Gate ===\n",
      "  Quality score: 92\n",
      "  Known issues:  ['Weekend gaps in timeseries — no weekend data points', 'Petrobras (CP006) rating may lag by 1 business day']\n",
      "\n",
      "=== Post-Fetch Validation ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [PASS] NOTIONAL > 0\n",
      "  [PASS] LGD in [0, 1]\n",
      "  [PASS] PD in [0, 1]\n",
      "  [PASS] EL approx NOTIONAL*LGD*PD\n",
      "\n",
      "All validations passed: True\n"
     ]
    }
   ],
   "source": [
    "# --- Ownership audit ---\n",
    "print(\"=== Domain Governance: credit ===\")\n",
    "desc = client.describe(\"credit\")\n",
    "own = desc.get(\"ownership\", {})\n",
    "print(f\"  Owner:   {own.get('accountable_owner', 'n/a')}\")\n",
    "print(f\"  Support: {own.get('support_channel', 'n/a')}\")\n",
    "\n",
    "# --- Lineage ---\n",
    "print(\"\\n=== Lineage: credit.exposures ===\")\n",
    "lineage = client.lineage(\"credit.exposures\")\n",
    "own_lin = lineage.get(\"ownership\", {})\n",
    "print(f\"  Owner:     {own_lin.get('accountable_owner')} (from {own_lin.get('accountable_owner_defined_at')})\")\n",
    "print(f\"  Hierarchy: {' -> '.join(lineage.get('path_hierarchy', []))}\")\n",
    "\n",
    "# --- Data quality gate ---\n",
    "print(\"\\n=== Data Quality Gate ===\")\n",
    "meta = client.metadata(\"credit.exposures\")\n",
    "dq = meta.data_quality or {}\n",
    "print(f\"  Quality score: {dq.get('quality_score', 'n/a')}\")\n",
    "print(f\"  Known issues:  {dq.get('known_issues', [])}\")\n",
    "\n",
    "# --- Post-fetch validation ---\n",
    "print(\"\\n=== Post-Fetch Validation ===\")\n",
    "val_result = client.fetch(\"credit.exposures\", limit=10000)\n",
    "df = pd.DataFrame(val_result.data)\n",
    "\n",
    "checks = {\n",
    "    \"NOTIONAL > 0\": (df[\"NOTIONAL\"] > 0).all(),\n",
    "    \"LGD in [0, 1]\": df[\"LGD\"].between(0, 1).all(),\n",
    "    \"PD in [0, 1]\": df[\"PD\"].between(0, 1).all(),\n",
    "    \"EL approx NOTIONAL*LGD*PD\": (\n",
    "        (df[\"EXPECTED_LOSS\"] - df[\"NOTIONAL\"] * df[\"LGD\"] * df[\"PD\"]).abs().max() < 1.0\n",
    "    ),\n",
    "}\n",
    "\n",
    "all_pass = True\n",
    "for check, passed in checks.items():\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "    if not passed:\n",
    "        all_pass = False\n",
    "    print(f\"  [{status}] {check}\")\n",
    "\n",
    "print(f\"\\nAll validations passed: {all_pass}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "---\n## Summary\n\n| Workflow | Source | Rows | Business Question |\n|---|---|---|---|\n| FI Relative Value | `fixed_income.govies/treasury` + `rates.swap` + `fixed_income.govies/sovereign` (Snowflake) | ~1,400 | Where are swap spreads wide and how do sovereign curves compare to UST? |\n| Credit Risk Dashboard | `credit.exposures` + `credit.limits` (MS-SQL) | ~960 + 32 | Which counterparties breach their single-name limits? |\n| MBS Pool Analysis | `reports/regulatory/2026Q1/summary` (Excel) | ~270 | How do prepayment speeds and balances vary across agencies and coupons? |\n| Discovery & Governance | Catalog API | -- | How do I find, inspect, and validate data before using it? |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}